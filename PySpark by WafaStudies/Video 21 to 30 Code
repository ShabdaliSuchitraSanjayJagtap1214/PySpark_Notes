Video 21. filter() & where() in PySpark
                -> PySpark filter() function is used to filter the rows from DataFrame based on the given condition or SQL expression.
                -> PySpark where() clause instead of the filter() if you are coming from an SQL background, both these functions operate exactly the same.
                -> For example sql querry [ select * from tblemployee where gender == 'male' ] this same thing we have to achive inside dataframe using pyspark than we used where() and filter() function.
                -> Both works simillary 

1. If you want to import everything from this library and start the spark session
      eg :- from pyspark.sql import *
            spark=SparkSession.builder.appName('Dataframe11').getOrCreate()
2. help(df.filter)
3. Created Dataframe.
      eg :- data = [(1,'shabdali',400000000),(2,'sarvesh',100000000),(3,'suchitra',200000000)]
            schema = ['id','name','salary']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema
            df.show()
4. We filter male employes and we get only male employes data as a output by using filter function.
      eg :- data = [(1,'shabdali',400000000),(2,'sarvesh',100000000),(3,'suchitra',200000000)]
            schema = ['id','name','salary']
            df = spark.createDataFrame(data,schema)
            df.filter(df.gender == 'male').show()
5. Another way to filter male employes and we get only male employes data as a output by using filter function.
      eg :- data = [(1,'shabdali',400000000),(2,'sarvesh',100000000),(3,'suchitra',200000000)]
            schema = ['id','name','salary']
            df = spark.createDataFrame(data,schema)
            df.filter("df.gender == 'male'").show()
6. We filter male employes and we get only male employes data as a output by using where function.
      eg :- data = [(1,'shabdali',400000000),(2,'sarvesh',100000000),(3,'suchitra',200000000)]
            schema = ['id','name','salary']
            df = spark.createDataFrame(data,schema)
            df.where("df.gender == 'male'").show()
7. We filter multiple records as we want all male employees and salary is 400000000 we get data as a output by using where function.
      eg :- data = [(1,'shabdali',400000000),(2,'sarvesh',100000000),(3,'suchitra',200000000)]
            schema = ['id','name','salary']
            df = spark.createDataFrame(data,schema)
            df.where((df.gender == 'male') & (df.salary == 400000000)).show()


Video 22. distinct() & dropDuplicates() in PySpark
                -> PySpark distinct() function is used to remove the duplicate rows (all columns).
                -> PySpark dropDuplicates() is used to drop rows based on selected (one or multiple) columns.
                -> So basically, using these functions we can get distinct rows.

1. Created Dataframe.
      eg :- data = [(1,'shabdali','female',400000000),(2,'sarvesh','male',100000000),(3,'suchitra','female',200000000)]
            schema = ['id','name','gender','salary']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema
            df.show()
2. We have duplicate records here and we want to remove all duplicate records and we want only distinct records so this will happends with the help of distinct() function.
      eg :- data = [(1,'shabdali','female',400000000),(2,'sarvesh','male',100000000),(2,'sarvesh','male',100000000),(3,'suchitra','female',200000000)]
            schema = ['id','name','gender','salary']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
3. help(df.distinct)
4. We have drop all duplicate records and we got all distinct records.
   We remove all record here but what if we want to remove only one column record  so this is happend using dropDuplicates() function. 
      eg :- data = [(1,'shabdali','female',400000000),(2,'sarvesh','male',100000000),(2,'sarvesh','male',100000000),(3,'suchitra','female',200000000)]
            schema = ['id','name','gender','salary']
            df = spark.createDataFrame(data,schema)
            df.distinct().show()
4. help(df.dropDuplicates)
5. We remove all record here but what if we want to remove only one column record  so this is happend using dropDuplicates() function. 
   This will apply on all columns.
      eg :- data = [(1,'shabdali','female',400000000),(2,'sarvesh','male',100000000),(2,'sarvesh','male',100000000),(3,'suchitra','female',200000000)]
            schema = ['id','name','gender','salary']
            df = spark.createDataFrame(data,schema)
            df.dropDuplicates().show()
6. We remove all record here but what if we want to remove only one column record  so this is happend using dropDuplicates() function. 
   This will apply only on gender column so we get only one column of male gender.
      eg :- data = [(1,'shabdali','female',400000000),(2,'sarvesh','male',100000000),(2,'sarvesh','male',100000000),(3,'suchitra','female',200000000)]
            schema = ['id','name','gender','salary']
            df = spark.createDataFrame(data,schema)
            df.dropDuplicates(['gender']).show()
7. We remove all record here but what if we want to remove only one column record  so this is happend using dropDuplicates() function. 
   This will apply only on 2 column.
      eg :- data = [(1,'shabdali','female',400000000),(2,'sarvesh','male',100000000),(2,'sarvesh','male',100000000),(3,'suchitra','female',200000000)]
            schema = ['id','name','gender','salary']
            df = spark.createDataFrame(data,schema)
            df.dropDuplicates(['gender','salary']).show()


Video 23. orderBy() & sort() in PySpark
                -> You can use either sort() or orderBy() function of PySpark DataFrame to sort DataFrame by ascending or descending order based on single or multiple columns. By default, sorting will happen in ascending order. We can explicitly mention ascending or descending using asc(), desc() functions.

1. Created Dataframe.
      eg :- data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema
            df.show()
2. help(df.sort)
3. We used sort function to sort dep column and by default it will sort in ascending order.
      eg :- data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.sort('dep').show()
4. Another way to used sort function to sort dep column and by default it will sort in ascending order.
      eg :- data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.sort(df.dep).show()
5. If we want to sort multiple columns using sort function to sort dep & id column and by default it will sort in ascending order.
      eg :- data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.sort(df.dep,df.id).show()
6. If we want to sort multiple columns using sort function to sort dep in ascending order & id in descending order.
      eg :- data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.sort(df.dep,df.id.desc()).show()
7. If we want to sort multiple columns using sort function to sort dep & id column and by default it will sort in ascending order.
      eg :-  data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.sort(df.dep.desc(),df.id.desc()).show()
8. We used orderBy function to sort dep column and by default it will sort in ascending order.
      eg :- data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.orderBy(df.dep).show()
9. We used both sort function and orderBy function to sort dep column and by default it will sort in ascending order.
      eg :- data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.sort(df.dep).show()
            df.orderBy(df.dep).show()
10. We used both sort function and orderBy function to sort dep column and id column.
      eg :- data = [(1,'shabdali','F',400000000,'Comp'),(2,'sarvesh','M',100000000,'Mech'),(3,'suchitra','F','HR'),(4,'sanjay','M','CEO')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.sort(df.dep.desc(),df.id.desc()).show()
            df.orderBy(df.dep.asc(),df.id.asc()).show()


Video 24. union() & unionAll() in PySpark
                -> union() and unionAll() transformations are used to merge two or more DataFrame's of the same schema or structure.
                -> union() & unionAll() method merges two DataFrames and returns the new DataFrame with all rows from two Dataframes regardless of duplicate data. 
                -> To remove duplicates use distinct() function.

1. Created 2 Dataframes. 
      eg :- data1 = [(1,'shabdali','Female'),(2,'sarvesh','Male')]
            schema1 = ['id','name','gender']
            data2 = [(3,'suchitra','Female'),(4,'sanjay','Male')]
            schema2 = ['id','name','gender']        
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            display(df1)
            df1.printSchema
            df1.show()
            display(df2)
            df2.printSchema
            df2.show()
            df2.show()
2. Using union() function we can merge two dataframes together.
      eg :- data1 = [(1,'shabdali','Female'),(2,'sarvesh','Male')]
            schema1 = ['id','name','gender']
            data2 = [(3,'suchitra','Female'),(4,'sanjay','Male')]
            schema2 = ['id','name','gender']
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            display(df1)
            df1.printSchema
            df1.show()
            display(df2)
            df2.printSchema
            df2.show()
            newDf = df1.union(df2)
            newDf.show()
3. Using union() function we can merge two dataframes together.
   We have duplicate records in dataframe but we can not remove duplicate records by using union() function we have to used distinct() & dropDuplicates() function only to remove duplicate records.
   But in SQL with the help of union() function we can remove duplicate records but not possible in PySpark.
      eg :- data1 = [(1,'shabdali','Female'),(2,'sarvesh','Male')]
            schema1 = ['id','name','gender']
            data2 = [(1,'shabdali','Female'),(3,'suchitra','Female'),(4,'sanjay','Male')]
            schema2 = ['id','name','gender']
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            display(df1)
            df1.printSchema
            df1.show()
            display(df2)
            df2.printSchema
            df2.show()
            newDf = df1.union(df2)
            newDf.show()
4. Using unionAll() function we can merge two dataframes together.
   We have duplicate records in dataframe but we can not remove duplicate records by using unionAll() function we have to used distinct() & dropDuplicates() function only to remove duplicate records.
   union() and unionAll() functions are same.
      eg :- data1 = [(1,'shabdali','Female'),(2,'sarvesh','Male')]
            schema1 = ['id','name','gender']
            data2 = [(1,'shabdali','Female'),(3,'suchitra','Female'),(4,'sanjay','Male')]
            schema2 = ['id','name','gender']         
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            display(df1)
            df1.printSchema
            df1.show()
            display(df2)
            df2.printSchema
            df2.show()
            newDf = df1.unionAll(df2)
            newDf.show()
5. help(df1.union)
6. We used distinct function here to remove duplicate records.
eg :- data1 = [(1,'shabdali','Female'),(2,'sarvesh','Male')]
            schema1 = ['id','name','gender']
            data2 = [(1,'shabdali','Female'),(3,'suchitra','Female'),(4,'sanjay','Male')]
            schema2 = ['id','name','gender']         
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            display(df1)
            df1.printSchema
            df1.show()
            display(df2)
            df2.printSchema
            df2.show()
            newDf = df1.unionAll(df2)
            newDf.distinct.show()


Video 25. groupBy count(),min(),max() in PySpark
                -> Similar to SQL GROUP BY clause, PySpark groupBy() function is used to collect the identical data into groups on DataFrame and perform count,sum, avg, min, max functions on the grouped data.

1. Creating Dataframe.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.show()
2. What type of object it will create we can see with the help of type function.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            print(type(df.groupBy('dep')))
3. We get new column name count and get a count on dep column where Comp has 1 count, Mech has 2 count, etc.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df1 = df.groupBy('dep').count()
            df1.show()
4. We get new columns and get min data of every columns.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.show()
            df.groupBy('dep').min().show()
5. We get new column name min(salary) and get a min salary on dep column where Comp has 70000000 salary, Mech has 50000000 salary, etc.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df1 = df.groupBy('dep').min('salary')
            df1.show()
6. We get new column name max(salary) and get a max salary on dep column where Comp has 70000000 salary, Mech has 60000000 salary, etc.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df1 = df.groupBy('dep').max('salary')
            df1.show()
7. We get new column name count and get a count on gender column where Comp has 1 female emp count, Mech has 1 male and 1 female emp, etc.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df1 = df.groupBy('dep','gender').count()
            df1.show()


Video 26. GroupBy agg() function in PySpark
                -> PySpark Groupby agg() is used to calculate more than one aggregate (multiple aggregates) at a time on grouped DataFrame.

1. Creating Dataframe.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.show()
2. help(df.groupBy('dep').agg)
3. from pyspark.sql.functions import count,min,max
   help(count)
4. If we want to apply all the functions like count, min, max on single dataframe than we used Aggregate Function.
   Here we had apply groupBy on dep column and used only one aggregate function.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.show()
            from pyspark.sql.functions import count,min,max
            df.groupBy('dep').agg(count('*').alias('countOfEmps')).show()
5. If we want to apply all the functions like count, min, max on single dataframe than we used Aggregate Function.
   Here we had apply groupBy on dep column and used 3 aggregate function.
      eg :- data = [(1,'shabdali','F',70000000,'Comp'),\
                    (2,'suchitra','F',60000000,'Mech'),\
                    (3,'Sarvesh','M',50000000,'Mech'),\
                    (4,'Sanjay','M',40000000,'CEO'),\
                    (5,'Ajay','M',30000000,'IT'),\
                    (6,'Sindu','F',20000000,'HR'),\
                    (7,'Vishnu','M',10000000,'IT')]
            schema = ['id','name','gender','salary','dep']
            df = spark.createDataFrame(data,schema)
            df.show()
            from pyspark.sql.functions import count,min,max
            df.groupBy('dep').agg(count('*').alias('countOfEmps'),\
                                  min('salary').alias('minSal'),\
                                  max('salary').alias('maxSal')).show()


Video 27. unionByName() function in PySpark
                -> unionByName() lets you to merge/union 2 DataFrames with a different number of columns (different schema) by passing allowMissingColumns with the value true.
                -> As we know union() & unionAll()  merge the 2 dataframe where the schema is same. Actually union() & unionAll() merge the data on column index that means column position.
                -> Where as unionByName() merge the data on column names.

1. Created 2 Dataframes.
      eg :- data1 = [(1,'shabdali','26')]
            schema1 = ['id','name','age']
            data2 = [(2,'sarvesh',20000000)]
            schema2 = ['id','name','salary']      
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            display(df1)
            df1.printSchema
            df1.show()
            display(df2)
            df2.printSchema
            df2.show()
2. Thow the column name is different than also with the help of union function we can merge the column because it merge schema on column indexs.
      eg :- data1 = [(1,'shabdali','26')]
            schema1 = ['id','name','age']
            data2 = [(2,'sarvesh',20000000)]
            schema2 = ['id','name','salary']         
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            df1.show()
            df2.show()
            df1.union(df2).show()
3. The column name is different by using unionByName function we can not merge the column because it merge schema on column names and column names are different.
   We got an error we can solve that error by using allowMissingColumns function.
      eg :- data1 = [(1,'shabdali','26')]
            schema1 = ['id','name','age']
            data2 = [(2,'sarvesh',20000000)]
            schema2 = ['id','name','salary']         
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            df1.show()
            df2.show()
            df1.unionByName(df2).show()
4. The column name is different by using unionByName function we can not merge the column because it merge schema on column names and column names are different.
   By passing allowMissingColumns function we had solve the error and can merge all the columns.
eg :- data1 = [(1,'shabdali','26')]
            schema1 = ['id','name','age']
            data2 = [(2,'sarvesh',20000000)]
            schema2 = ['id','name','salary']         
            df1 = spark.createDataFrame(data1,schema1)
            df2 = spark.createDataFrame(data2,schema2)
            df1.show()
            df2.show()
            df1.unionByName(df2, allowMissingColumns = True).show()



