Video 2. Create Dataframe Manually :-

1.  If you want to import everything from this library and start the spark session :-
      from pyspark.sql import SparkSession
      spark=SparkSession.builder.appName('Dataframe').getOrCreate()
2. type() Function :- This function will know the Datatype of any value or data.
      eg :- type('abcd')
3. dir() Function :-  This function give you all the list of all Attributes or Properties and Methods or Functions available on top of that objects.
      eg :- dir(spark)
4. help() Function :- This function help us how we can write a any syntax.
      eg :- help(spark.createDataFrame)
5. show() Function :- This function is used to display the output in table form and show Max 20 Characters and will show Max 20 Rows Only.
      eg :- df.show()
6. spark.createDataFrame() Function :- Creating Dataframe with the help of List and Column Name
      eg :- data = [(1,'Shabdali'),(2,'Jagtap')]
            df = spark.createDataFrame(data=data,schema=['id','name'])
            df.show()
7. printSchema() Function :- This function is used to get Schema of this Dataframe or Schema of the Table and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali'),(2,'Jagtap')]
            df = spark.createDataFrame(data=data,schema=['id','name'])
            df.show()
            df.printSchema()

8. StructType() Function :- Creating Dataframe with the help of List and Column Name and to convert id Datatype from Long to Integer.
      eg :- data = [(1,'Shabdali'),(2,'Jagtap')]
            schema = StructType([StructField(name='id',dataType=IntegerType()),
                                 StructField(name='name',dataType=StringType())])
            df = spark.createDataFrame(data=data,schema=schema)
            df.show()
            df.printSchema()

9. Creating Dataframe with the help of Dictionary and we get Datatype of id is Long.
      eg :- Data = [{'id':1, 'name':'Shabdali'},
                    {'id':2, 'name':'Sanjay'}]
            df = spark.createDataFrame(data=data)
            df.show()
            df.printSchema()


Video 3. Read CSV File in to Dataframe :-

1. help() Function :- This function help us how we can write a any syntax.
      eg :- help(spark)
      eg :- help(spark.read)
2. show() Function :- This function is used to display the output in table form and show Max 20 Characters and will show Max 20 Rows Only.
      eg :- df.show()
3. display() Function :- This function is used to display the output in dataframe form.
      eg :- display(df)
4. Reading Single CSV File and Getting wrong Header
      eg :- df = spark.read.csv(path=r'C:\Users\progr\AFTP\PySpark\test1.csv')
            df.show()
            display(df)
            df.printSchema()
5. Reading Single CSV File and Getting correct Header
      eg :- df = spark.read.csv(path=r'C:\Users\progr\AFTP\PySpark\test1.csv',header=True)
            df.show()
            display(df)
            df.printSchema()
6. Another way Reading Single CSV File and Getting correct Header
      eg :- df = spark.read.format('csv').option(key='header',value=True).load(path=r'C:\Users\progr\AFTP\PySpark\test1.csv')
            df.show()
            display(df)
            df.printSchema()
7. help() Function :- This function help us how we can write a any syntax.
      eg :- help(spark.read.csv) -: We get various parameter here that we can use as we had used Header.
8. Reading Multiple CSV File from Same Path/Location and Getting correct Header
      eg :- df = spark.read.csv(path=[r'C:\Users\progr\AFTP\csv_folder\test1.csv',r'C:\Users\progr\AFTP\PySpark\csv_folder\test2.csv'],header=True)
            df.show()
            display(df)
            df.printSchema()
9. Reading Multiple CSV File from different Path/Location and Getting correct Header
      eg :- df = spark.read.csv(path=r'C:\Users\progr\AFTP\PySpark\csv_folder\',header=True)
            df.show()
            display(df)
            df.printSchema()
10. help() Function :- This function help us how we can write a any syntax.
      eg :- help(StructType)
11. StructType() Function :- This function is used to convert one Datatype to another Datatype.
      eg :- from pyspark.sql.types import *
            schema = StructType().add(field='Name',data_type=StringType())\
                                 .add(field='age',data_type=IntegerType())
            df = spark.read.csv(path=r'C:\Users\progr\AFTP\csv_folder\test1.csv',schema=schema,header=True)
            df.show()
            display(df)
            df.printSchema()


Video 4. Write Dataframe into CSV File using PySpark :-

1. If you want to import everything from this library and start the spark session :-
      from pyspark.sql import *
2. help() Function :- This function help us how we can write a any syntax.
      eg :- help(DataFrameWriter)
3. help() Function :- This function help us how we can write a any syntax.
      eg :- help(DataFrame.write) / help(dataframe.DataFrame.write) 
4. spark.createDataFrame() Function :- Creating Dataframe with the help of List and Column Name
      eg :- data = [(1,'Shabdali'),(2,'Sanjay')]
            schema = ['id','name']
            df = spark.createDataFrame(data = data, schema = schema)
            display(df)
            df.show()
5. type() Function :- This function will know the Datatype of any value or data.
      eg :- type(df.write)
6. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.write)
7. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.write.csv)
8. We Write Dataframe into CSV File in that given location and will create Folder in the given location and if same folder is there in that path than it will write in that folder without creating.
      eg :- df.write.csv(path=r'C:\Users\progr\AFTP\PySpark\CSV\Test3',header=True)
9. This Read Function is used to read that Dataframe.
      eg :- display(spark.read.csv(path=r'C:\Users\progr\AFTP\PySpark\CSV\Test3',header=True))
10. By using Mode the error will pop up if file is already exits.
      eg :- df.write.csv(path=r'C:\Users\progr\AFTP\PySpark\CSV\Test3',header=True,mode='error')
11. By using Mode the error will pop up if file is already exits and we want to ignore error than used mode function.
      eg :- df.write.csv(path=r'C:\Users\progr\AFTP\PySpark\CSV\Test3',header=True,mode='ignore')
12. By using Mode we can Append/Add on existing data. So Multiple time we run this command than data will add again and again.
      eg :- df.write.csv(path=r'C:\Users\progr\AFTP\PySpark\CSV\Test3',header=True,mode='append')
13. By using Mode we can overwrite on existing data.
      eg :- df.write.csv(path=r'C:\Users\progr\AFTP\PySpark\CSV\Test3',header=True,mode='overwrite')
            # To read data again to check that data is overwrite or not and this Read Function is used to read that Dataframe.
            display(spark.read.csv(path=r'C:\Users\progr\AFTP\PySpark\CSV\Test3',header=True))


Video 5. Read JSON File into Dataframe

1. help() Function :- This function help us how we can write a any syntax.
      eg :- help(spark.read)
2. help() Function :- This function help us how we can write a any syntax.
      eg :- help(spark.read.json)
3. printSchema() Function :- This function is used to get Schema of this Dataframe or Schema of the Table and we get Datatype of id is Long.
      eg :- df.printSchema()
4. Reading SingleLine JSON File/Dataframe from same File Path/Location.
      eg :- df = spark.read.json(path=r'C:\Users\progr\AFTP\PySpark\csv_json_folder_1\JSONSingleLineFile1.json')
            df.printSchema()
            df.show()
5. Reading MultiLine JSON File/Dataframe from same File Path/Location and We had used MultiLine Function.
      eg :- df = spark.read.json(path=r'C:\Users\progr\AFTP\PySpark\csv_json_folder_1\JSONSingleLineFile1.json',multiLine=True)
            df.printSchema()
            df.show()
6. Reading SingleLine JSON File/Dataframe from Different File Path/Location.
      eg :- df = spark.read.json(path=[r'C:\Users\progr\AFTP\PySpark\csv_json_folder_1\JSONSingleLineFile1.json',
                                       r'C:\Users\progr\AFTP\PySpark\csv_json_folder_2\JSONSingleLineFile2.json'])
            df.printSchema()
            df.show()
7. Reading MultiLine JSON File/Dataframe from Different File Path/Location.
      eg :- df = spark.read.json(path=[r'C:\Users\progr\AFTP\PySpark\csv_json_folder_1\JSONMultiLineFile1.json',
                           r'C:\Users\progr\AFTP\PySpark\csv_json_folder_2\JSONMultiLineFile2.json',multiLine=True])
            df.printSchema()
            df.show()
8. Reading 2 or more SingleLine JSON File/Dataframe from same File Path/Location with the help of *.
      eg :- df = spark.read.json(path=r'C:\Users\progr\AFTP\PySpark\csv_json_folder_3\*.json')
            df.printSchema()
            df.show()
9. Reading SingleLine JSON File/Dataframe from same File Path/Location and Convert Datatype from String to Integer.
      eg :- from pyspark.sql import *
            schema = StructType().add(field='id',data_type=IntegerType())\
                                 .add(field='name',data_type=StringType())\
                                 .add(field='gender',data_type=StringType())\
                                 .add(field='salary',data_type=IntegerType())
                     df = spark.read.json(path=r'C:\Users\progr\AFTP\PySpark\csv_json_folder_1\JSONSingleLineFile1.json', schema=schema)
                     df.printSchema()
                     df.show()


Video 6. Write Dataframe into JSON File using PySpark
            -> Use the DataFrameWriter object to write PySpark DataFrame to a JSON File. 

1. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali'),(2,'Sanjay')]
            schema = ['id','name']
            df = spark.createDataFrame(data=data,schema=schema)
            display(df)
            df.show()

2. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.write)

3. We Write Dataframe into CSV File in that given location and will create Folder in the given location and if same folder is there in that path than it will write in that folder without creating.
   In this Data will store as a Part Files.
      eg :- df.write.json(r'C:\Users\progr\AFTP\PySpark\jsondata\emps.json')
                                    or
            df.write.json(r'C:\Users\progr\AFTP\PySpark\jsondata\emps')
4. This Read Function is used to read that Dataframe in JSON File.
      eg :- display(spark.read.json(r'C:\Users\progr\AFTP\PySpark\jsondata\emps'))
5. By using Mode the error will pop up if file is already exits.
      eg :- df.write.json(r'C:\Users\progr\AFTP\PySpark\jsondata\emps',mode='error')
6. By using Mode the error will pop up if file is already exits and we want to ignore error than used mode function.
      eg :- df.write.json(r'C:\Users\progr\AFTP\PySpark\jsondata\emps',mode='ignore')
7. By using Mode we can Append/Add on existing data. So Multiple time we run this command than data will add again and again.
      eg :- df.write.json(r'C:\Users\progr\AFTP\PySpark\jsondata\emps',mode='append')
8. By using Mode we can overwrite on existing data.
      eg :- df.write.json(r'C:\Users\progr\AFTP\PySpark\jsondata\emps',mode='overwrite')
            # To read data again to check that data is overwrite or not and this Read Function is used to read that Dataframe.
            display(spark.read.json(r'C:\Users\progr\AFTP\PySpark\jsondata\emps'))


Video 7. Read Parquet File into Dataframe using PySpark
            -> Using read.parquet("path") or read.format("parquet").load("path") you can read a Parquet File into a PySpark Dataframe.

1.  help() Function :- This function help us how we can write a any syntax.
      eg :- help(spark.read.parquet)
2. Reading Single Parquet File/Dataframe from same File Path/Location and used Count function to see how many records are there.
      eg :- df = spark.read.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder\userdata1.parquet')
            display(df)
            df.show()
            print(df.count())
3. Used Count function to see how many records are there in the file.
      eg :- print(df.count())
4. Reading Multiple Parquet File from Same Path/Location with the help of *.
      eg :- df = spark.read.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder\*.parquet')
            display(df)
            df.show()
            print(df.count())
5. Reading Multiple Parquet File from Same Path/Location where only Parquet file are prsent so no need of *.parquet 
      eg :- df = spark.read.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder\')
            display(df)
            df.show()
            print('Count of Recordes in Dataframe is ' + str (df.count()))


Video 8. Write Dataframe into Parquet File using PySpark
            -> Use the DataFrameWriter object to write PySpark DataFrame to a Parquet File.

1. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali'),(2,'Sanjay')]
            schema = ['id','name']
            df = spark.createDataFrame(data=data,schema=schema)
            display(df)
            df.show()
2. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.write)
3. We Write Dataframe into Parquet File in that given location and will create Folder in the given location and if same folder is there in that path than it will write in that folder without creating.
   In this Data will store as a Part Files
      eg :- df.write.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder/parquetoutput.parquet')
4. Another wayt to Write Dataframe into Parquet File in that given location and will create Folder in the given location and if same folder is there in that path than it will write in that folder without creating.
   In this Data will store as a Part Files
      eg :- df.write.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder/parquetoutput')
            # This Read Function is used to read that Dataframe in JSON File.
            df1 = spark.read.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder/parquetoutput')
            display(df1)
5. By using Mode the error will pop up if file is already exits.
      eg :- df.write.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder/parquetoutput',mode='error')
6. By using Mode the error will pop up if file is already exits and we want to ignore error than used mode function.
      eg :- df.write.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder/parquetoutput',mode='ignore')
7. By using Mode we can Append/Add on existing data. So Multiple time we run this command than data will add again and again.
      eg :- df.write.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder/parquetoutput',mode='append') 
8. Byusing Mode we can overwrite on existing data.
      eg :- df.write.parquet(r'C:\Users\progr\AFTP\PySpark\parquet_folder/parquetoutput',mode='overwrite')


Video 9. show() in PySpark to display Dataframe contents in Table 
            -> show() is used to display contents in Table.

1. show() Function :- This function is used to display the output in table form and show Max 20 Characters and will show Max 20 Rows Only.
      eg :- data = [(1,'hdadjsdjshdjshdjskndndsm'),
                    (2,'kjsdhasg hdajsd ashjds hdghgd'),
                    (3,'sjdjsghdgdajkdsjmndms'),
                    (4,'ksdkjskdkjksdk')]
            schema = ['id','comments']
            df = spark.createDataFrame(data=data,schema=schema)
            df.show()
2. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.show)
3. This show function is used to display the output in table form and with the help of truncate function than it will show All Characters.
      eg :- data = [(1,'hdadjsdjshdjshdjskndndsm'),
                    (2,'kjsdhasg hdajsd ashjds hdghgd'),
                    (3,'sjdjsghdgdajkdsjmndms'),
                    (4,'ksdkjskdkjksdk')]
            schema = ['id','comments']
            df = spark.createDataFrame(data=data,schema=schema)
            df.show(truncate=False)
4. This show function is used to display the output in table form and with the help of truncate function we can show only 8 Characters.
      eg :- data = [(1,'hdadjsdjshdjshdjskndndsm'),
                    (2,'kjsdhasg hdajsd ashjds hdghgd'),
                    (3,'sjdjsghdgdajkdsjmndms'),
                    (4,'ksdkjskdkjksdk')]
            schema = ['id','comments']
            df = spark.createDataFrame(data=data,schema=schema)
            df.show(truncate=8)
5. This show function is used to display the output in table form and with the help of truncate function we can show only 8 Characters and with the help of n function we can show only 2 Rows.
      eg :- data = [(1,'hdadjsdjshdjshdjskndndsm'),
                    (2,'kjsdhasg hdajsd ashjds hdghgd'),
                    (3,'sjdjsghdgdajkdsjmndms'),
                    (4,'ksdkjskdkjksdk')]
            schema = ['id','comments']
            df = spark.createDataFrame(data=data,schema=schema)
            df.show(n=2,truncate=8)
6. This show function is used to display the output in table form and with the help of truncate function we can show only 8 Characters and with the help of n function we can show only 2 Rows.
   If we want to show Data in Vertical Manner by using Vertical function.
      eg :- data = [(1,'hdadjsdjshdjshdjskndndsm'),
                    (2,'kjsdhasg hdajsd ashjds hdghgd'),
                    (3,'sjdjsghdgdajkdsjmndms'),
                    (4,'ksdkjskdkjksdk')]
            schema = ['id','comments']
            df = spark.createDataFrame(data=data,schema=schema)
            df.show(n=2, truncate=8, vertical=True)


Video 10. withColumn() in PySpark will Add new column or Change existing column Data or Type in Dataframe.
            -> PySpark withColumn() is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more.

1. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            display(df)
            df.show()
            df.printSchema()
2. Display Functions 
      eg :- display(df)
      eg :- df.show()
      eg :- df.printSchema()
3. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.withColumn)
4. If you want to import col functions from this library :-
      eg :- from pyspark.sql.functions import col
      eg :- from pyspark.sql.functions import col,lit
5. withColumn function will Add new column or Change existing column Data or Type in Dataframe.
   We import col
   We want to change Datatype of Salary column from String to Integer and change will store in new Dataframe or you can save into same Dataframe also.
      eg :- from pyspark.sql.functions import col
            data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df1 = df.withColumn(colName='salary',col=col('salary').cast('Integer'))
            display(df1)
            df1.show()
            df1.printSchema()
6. withColumn function will Add new column or Change existing column Data or Type in Dataframe.
   We import col
   We want to change Data of Salary column from 3000*2 = 6000 & 4000*2 = 8000 and change will update on all column and change will store in new Dataframe or you can save into same Dataframe also.
      eg :- from pyspark.sql.functions import col
            data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df2 = df1.withColumn('salary',col('salary') * 2)
            display(df2)
            df2.show()
            df2.printSchema()
7. withColumn function will Add new column or Change existing column Data or Type in Dataframe.
   We import col,lit  
   It will check first column is there in that table or not if column is not there than it will create new column otherwise if column is exists it will make changes in existing column.
   country column will create and india will add as a records in country column.
      eg :- from pyspark.sql.functions import col,lit
            data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df3 = df2.withColumn('country',lit('india'))
            display(df3)
            df3.show()
            df3.printSchema()
8. withColumn function will Add new column or Change existing column Data or Type in Dataframe.
   We import col,lit  
   New column is create with the copy/same of records/data from existing column.
      eg :- from pyspark.sql.functions import col,lit
            data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df4 = df3.withColumn('copiedSalary',col('salary'))
            display(df4)
            df4.show()
            df4.printSchema()




