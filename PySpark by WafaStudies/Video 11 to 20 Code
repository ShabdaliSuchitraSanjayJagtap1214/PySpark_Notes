Video 11. withColumnRenamed() usage in PySpark
                -> PySpark withColumnRenamed() is a Transformation Function of Dataframe which is used to change existing column name in Dataframe.
â€‹
1. If you want to import everything from this library and start the spark session
      eg :- from pyspark.sql import *
            spark=SparkSession.builder.appName('Dataframe11').getOrCreate()
2. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            display(df)
            df.show()
            df.printSchema()
3. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.withColumnRenamed)
4. Dataframe are immutable it means we can not change existing dataframe df and always whatever we function applied it will actually gunerate another Dataframe.
   Checking that it is realy immutable or not and the name of salary column has not change
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df.withColumnRenamed('salary','salary_amount')
            display(df)
            df.show()
            df.printSchema()
5. We rename the salary column name to salary_amount and updated data will store in new dataframe df1.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df1 = df.withColumnRenamed('salary','salary_amount')
            display(df1)
            df1.show()
            df1.printSchema()



Videp 12. StructType() and StructField() in PySpark
                -> PySpark StructField classes are used to programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns.
                -> PySpark StructType is a collection of StructField's.

1. Creating Dataframe with the help of List and but we do not have Column Name because we do not define schema and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            df = spark.createDataFrame(data)
            display(df)
            df.printSchema()
            df.show()
2. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data,columns)
            display(df)
            df.printSchema()
            df.show()
3. If you want to import StructType, StructField, StringType, IntegerType from the library
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
4. Creating Dataframe with the help of List and Column Name by using StructType and StructField Function and define Datatype as well.
   Another way to create Dataframe.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
            data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            schema = StructType([\ StructField(name='id',dataType=IntegerType()),
                                 \ StructField(name='name',dataType=StringType()),
                                 \ StructField(name='salary',dataType=IntegerType())])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
5. Creating Dataframe with the help of List and Column Name by using StructType and StructField Function and define Datatype as well.
   Another way to create Dataframe.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
            data = [(1,('Shabdali','Jagtap'),'30000000'),(2,('Sanjay','Patil'),'10000000')]
            structName = StructType([\ StructField('firstName',StringType()),
                                     \ StructField('lastName',StringType())])
            schema = StructType([\ StructField(name='id',dataType=IntegerType()),
                                 \ StructField(name='name',dataType=structName),
                                 \ StructField(name='salary',dataType=IntegerType())])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()


Video 13. ArrayType Columns in PySpark
                -> It is like a List in Python

1. Creating Dataframe with the help of List and Column Name and numbers datatype is array and will detacked automatically by createDataFrame function.
   In this we can see numbers is arrry type but element is long datatype and we want it as Integer type.
      eg :- data = [('abc',[1,2]),('mno',[4,5]),('xyz',[7,8])]
            schema = ['id','numbers']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()

2. We Import StructType, StructField, StringType, IntegerType. We also Import ArrayType, StringType, IntegerType.
   Used StructType, StructField function to convert element datatype from Long to Integer.
   It is used to design/create complex Dataframes.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
            schema = StructType([\ StructField('id',StringType()),
                                 \ StructField('numbers',ArrayType(IntegerType()))])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
3. help() Function :- This function help us how we can write a any syntax.
      eg :- help(ArrayType) 
4. It will creat new column with the name firstNumber and will display 0th Index position value of an array. So we can fetch array elements with the help of Indexes.
      eg :- df.withColumn('firstNumber', df.numbers[0]).show()
5. In this we created 2 column num1 and num2 & data is not in array form so will store in seperate order like num1 = 1 and 3 and in num2 = 2 and 4.
      eg :- data = [(1,2),(3,4)]
            schema = ['num1','num2']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()    
6. In this we created 2 column num1 and num2 & data is not in array form so will store in seperate order like num1 = 1 and 3 and in num2 = 2 and 4.
   If we want to combine into an array form and make new column to store that data with the name numbers.
   And withColumn function help us to create new column.
      eg :- from pyspark.sql.functions import col,array
            data = [(1,2),(3,4)]
            schema = ['num1','num2']
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('numbers', array(col('num1'),col('num2')))
            display(df1)
            df1.printSchema()
            df1.show()
7. If you want to import col,array from the library.
      eg :- from pyspark.sql.functions import col,array


Video 14. explode(), split(), array() & array_contains() functions in PySpark
              -> explode() is a function to create a new row for each element in the given array column.
              -> split() is sql function returns an array type after splitting the string column by delimiter.
              -> array() is function to create a new array column by merging the data from multiple columns.

1. Creating Dataframe having 3 columns and skill column has 2 elements like shabdali has python a 2 elements like python and azure and same for sanjay.
      eg :- data = [(1,'shabdali',['python','azure']),(2,'sanjay',['java','aws'])]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
2. If you want to import explode,col from the library.
      eg :- from pyspark.sql.functions import explode,col
3. Creating Dataframe having 3 columns and skill column has 2 elements like shabdali has python a 2 elements like python and azure and same for sanjay.
   We want elements should be called seperatly in new rows like 1st row shabdali python 2nd row shabdali azure and so on. Then we need explode function.
   explode is a function to create a new row for each element in the given array column.
      eg :- from pyspark.sql.functions import explode,col
            data = [(1,'shabdali',['python','azure']),(2,'sanjay',['java','aws'])]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('SkillExplode',explode(col('skills'))) 
            display(df1)
            df1.printSchema()
            df1.show()
4. In the output the python and azure is same column and not in array/list fashion [].
   We have to splite the element which are seperated by ',' in new dataframe and in new column and store in array/list fashion [] so it will done by splite function.
      eg :- data = [(1,'shabdali','python,azure'),(2,'sanjay','java,aws')]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
5. If you want to import col,array from the library.
      eg :- from pyspark.sql.functions import split,col
6. We have to splite the element which are seperated by ',' in new dataframe and in new column and store in array/list fashion [] so it will done by splite function.
   split() is sql function returns an array type after splitting the string column by delimiter.
   Records are in array/list fashion [].
      eg :- from pyspark.sql.functions import split,col
            data = [(1,'shabdali','python,azure'),(2,'sanjay','java,aws')]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('skillsSplit',split(col('skills'),',')) 
            display(df1)
            df1.printSchema()
            df1.show()

