  Video 11. withColumnRenamed() usage in PySpark
                -> PySpark withColumnRenamed() is a Transformation Function of Dataframe which is used to change existing column name in Dataframe.
â€‹
1. If you want to import everything from this library and start the spark session
      eg :- from pyspark.sql import *
            spark=SparkSession.builder.appName('Dataframe11').getOrCreate()
2. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            display(df)
            df.show()
            df.printSchema()
3. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.withColumnRenamed)
4. Dataframe are immutable it means we can not change existing dataframe df and always whatever we function applied it will actually gunerate another Dataframe.
   Checking that it is realy immutable or not and the name of salary column has not change
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df.withColumnRenamed('salary','salary_amount')
            display(df)
            df.show()
            df.printSchema()
5. We rename the salary column name to salary_amount and updated data will store in new dataframe df1.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df1 = df.withColumnRenamed('salary','salary_amount')
            display(df1)
            df1.show()
            df1.printSchema()



Videp 12. StructType() and StructField() in PySpark
                -> PySpark StructField classes are used to programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns.
                -> PySpark StructType is a collection of StructField's.

1. Creating Dataframe with the help of List and but we do not have Column Name because we do not define schema and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            df = spark.createDataFrame(data)
            display(df)
            df.printSchema()
            df.show()
2. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data,columns)
            display(df)
            df.printSchema()
            df.show()
3. If you want to import StructType, StructField, StringType, IntegerType from the library
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
4. Creating Dataframe with the help of List and Column Name by using StructType and StructField Function and define Datatype as well.
   Another way to create Dataframe.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
            data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            schema = StructType([\ StructField(name='id',dataType=IntegerType()),
                                 \ StructField(name='name',dataType=StringType()),
                                 \ StructField(name='salary',dataType=IntegerType())])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
5. Creating Dataframe with the help of List and Column Name by using StructType and StructField Function and define Datatype as well.
   Another way to create Dataframe.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
            data = [(1,('Shabdali','Jagtap'),'30000000'),(2,('Sanjay','Patil'),'10000000')]
            structName = StructType([\ StructField('firstName',StringType()),
                                     \ StructField('lastName',StringType())])
            schema = StructType([\ StructField(name='id',dataType=IntegerType()),
                                 \ StructField(name='name',dataType=structName),
                                 \ StructField(name='salary',dataType=IntegerType())])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()


Video 13. ArrayType Columns in PySpark
                -> It is like a List in Python

1. Creating Dataframe with the help of List and Column Name and numbers datatype is array and will detacked automatically by createDataFrame function.
   In this we can see numbers is arrry type but element is long datatype and we want it as Integer type.
      eg :- data = [('abc',[1,2]),('mno',[4,5]),('xyz',[7,8])]
            schema = ['id','numbers']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()

2. We Import StructType, StructField, StringType, IntegerType. We also Import ArrayType, StringType, IntegerType.
   Used StructType, StructField function to convert element datatype from Long to Integer.
   It is used to design/create complex Dataframes.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
            schema = StructType([\ StructField('id',StringType()),
                                 \ StructField('numbers',ArrayType(IntegerType()))])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
3. help() Function :- This function help us how we can write a any syntax.
      eg :- help(ArrayType) 
4. It will creat new column with the name firstNumber and will display 0th Index position value of an array. So we can fetch array elements with the help of Indexes.
      eg :- df.withColumn('firstNumber', df.numbers[0]).show()
5. In this we created 2 column num1 and num2 & data is not in array form so will store in seperate order like num1 = 1 and 3 and in num2 = 2 and 4.
      eg :- data = [(1,2),(3,4)]
            schema = ['num1','num2']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()    
6. In this we created 2 column num1 and num2 & data is not in array form so will store in seperate order like num1 = 1 and 3 and in num2 = 2 and 4.
   If we want to combine into an array form and make new column to store that data with the name numbers.
   And withColumn function help us to create new column.
      eg :- from pyspark.sql.functions import col,array
            data = [(1,2),(3,4)]
            schema = ['num1','num2']
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('numbers', array(col('num1'),col('num2')))
            display(df1)
            df1.printSchema()
            df1.show()
7. If you want to import col,array from the library.
      eg :- from pyspark.sql.functions import col,array


Video 14. explode(), split(), array() & array_contains() functions in PySpark
              -> explode() is a function to create a new row for each element in the given array column.
              -> split() is sql function returns an array type after splitting the string column by delimiter.
              -> array() is function to create a new array column by merging the data from multiple columns.

1. Creating Dataframe having 3 columns and skill column has 2 elements like shabdali has python a 2 elements like python and azure and same for sanjay.
      eg :- data = [(1,'shabdali',['python','azure']),(2,'sanjay',['java','aws'])]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
2. If you want to import explode,col from the library.
      eg :- from pyspark.sql.functions import explode,col
3. Creating Dataframe having 3 columns and skill column has 2 elements like shabdali has python a 2 elements like python and azure and same for sanjay.
   We want elements should be called seperatly in new rows like 1st row shabdali python 2nd row shabdali azure and so on. Then we need explode function.
   explode is a function to create a new row for each element in the given array column.
      eg :- from pyspark.sql.functions import explode,col
            data = [(1,'shabdali',['python','azure']),(2,'sanjay',['java','aws'])]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('SkillExplode',explode(col('skills'))) 
            display(df1)
            df1.printSchema()
            df1.show()
4. In the output the python and azure is same column and not in array/list fashion [].
   We have to splite the element which are seperated by ',' in new dataframe and in new column and store in array/list fashion [] so it will done by splite function.
      eg :- data = [(1,'shabdali','python,azure'),(2,'sanjay','java,aws')]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
5. If you want to import split,col from the library.
      eg :- from pyspark.sql.functions import split,col
6. We have to splite the element which are seperated by ',' in new dataframe and in new column and store in array/list fashion [] so it will done by splite function.
   split() is sql function returns an array type after splitting the string column by delimiter.
   Records are in array/list fashion [].
      eg :- from pyspark.sql.functions import split,col
            data = [(1,'shabdali','python,azure'),(2,'sanjay','java,aws')]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('skillsSplit',split(col('skills'),',')) 
            display(df1)
            df1.printSchema()
            df1.show()
7. We dont get new column is form of Array Style.
   If we want to create a new array column by merging the data from multiple columns that will done by using Array function.
      eg :- data = [(1,'shabdali','python','azure'),(2,'sanjay','java','aws')]
            schema = ['id','name','primarySkills','secondarySkills']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
8. We get new column name skillsArray and with combine data of to columns primarySkill and secondarySkill in the array/list formate.
   array() is function to create a new array column by merging the data from multiple columns.
      eg :- from pyspark.sql.functions import array,col
            data = [(1,'shabdali','python','azure'),(2,'sanjay','java','aws')]
            schema = ['id','name','primarySkill','secondarySkill']
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('skillsArray',array(col('primarySkill'),col('secondarySkill'))) 
            display(df1)
            df1.printSchema()
            df1.show()
9. If you want to import array,col from the library.
      eg :- from pyspark.sql.functions import array,col
10. If we want to see who has Python skill set and want that data in new column and it will say True if the person has python skill set otherwise says False.
    This will happend by using array_contains function.
      eg :- data = [(1,'shabdali',['python','azure']),(2,'sanjay',['java','aws'])]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
11. If we want to see who has Python skill set and want that data in new column and it will say True if the person has python skill set otherwise says False.
    array_contains() is sql function is used to check if array column contains a value. Returns null if the array is null, true if the array contains the value, and false otherwise.
      eg :- from pyspark.sql.functions import array_contains,col
            data = [(1,'shabdali',['python','azure']),(2,'sanjay',['java','aws'])]
            schema = ['id','name','skills']
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('HasArray_ContainsPythonSkills',array_contains(col('skills'),'python')) 
            display(df)
            df.printSchema()
            df.show()
12. If you want to import array_contains,col from the library.
      eg :- from pyspark.sql.functions import array_contains,col


Video 15. MapType Column in PySpark
                -> PySpark MapType is used to represent map key-value pair similar to python Dictionary (Dict).

1. We make Dataframe in Dictionary {} formate. So createDataFrame detected that properties is map datatype. It gives Key Value Paire.
   So datatype of Dictionary Data is Map.
   We used truncate = False because we want to see all the Data.
      eg :- data = [('shabdali',{'hair':'black','eye':'brown'}),('sanjay',{'hair':'brown','eye':'blue'})]
            schema = ['name','properties']
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show(truncate=False)
2. If you want to import StructType, StructField, StringType, MapType from the library.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, MapType
3. help() Function :- This function help us how we can write a any syntax.
      eg :- help(MapType)
4. Same Example with the help of StructType, StructField.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, MapType
            data = [('shabdali',{'hair':'black','eye':'brown'}),('sanjay',{'hair':'brown','eye':'blue'})]
            schema = schema = StructType([\ StructField('name',StringType()),
                                          \ StructField('properties',MapType(StringType(),StringType()))])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            # df.show()
            df.show(truncate=False)
5. How to Access MapType Elements 
   We acess hire elements by creating new column with the help of withColumn function and access the elements in that new column name hair.
       eg :- from pyspark.sql.types import StructType, StructField, StringType, MapType
            data = [('shabdali',{'hair':'black','eye':'brown'}),('sanjay',{'hair':'brown','eye':'blue'})]
            schema = schema = StructType([\ StructField('name',StringType()),
                                          \ StructField('properties',MapType(StringType(),StringType()))])
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('hire',df.properties['hair']) 
            display(df1)
            df1.printSchema()
            df1.show(truncate=False)
6. How to Access MapType Elements 
   We acess hire elements by creating new column with the help of withColumn function and access the elements in that new column name hair.
   Same as hair we can access eye elements also.
   we can used any syntax df1 = df.withColumn('eye',df.properties['eye']) or df1 = df.withColumn('eye',df.properties.getItem['eye']) 
      eg :- from pyspark.sql.types import StructType, StructField, StringType, MapType
            data = [('shabdali',{'hair':'black','eye':'brown'}),('sanjay',{'hair':'brown','eye':'blue'})]
            schema = schema = StructType([\ StructField('name',StringType()),
                              \ StructField('properties',MapType(StringType(),StringType()))])
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('eye',df.properties.getItem['eye']) 
            display(df1)
            df1.printSchema()
            df1.show(truncate=False)


Video 16. map_keys(), map_values() & explode() functions to work with MapType Columns in PySpark

1. We make Dataframe in Dictionary {} formate.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, MapType
            data = [('shabdali',{'hair':'black','eye':'brown'}),('sanjay',{'hair':'brown','eye':'blue'})]
            schema = StructType([\ StructField('name',StringType()),
                                 \ StructField('properties',MapType(StringType(),StringType()))])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show(truncate=False)
2. It will get 2 new column name key and value. key column holds all the keys and value holds all the values.
      eg :- from pyspark.sql.functions import explode
            data = [('shabdali',{'hair':'black','eye':'brown'}),('sanjay',{'hair':'brown','eye':'blue'})]
            schema = StructType([\ StructField('name',StringType()),
                                 \ StructField('properties',MapType(StringType(),StringType()))])
            df = spark.createDataFrame(data,schema)  
            df1 = df.select('name','properties',explode(df.properties))
            display(df1)
            df1.printSchema()
            df1.show(truncate=False)
3. If you want to import explode from the library.
      eg :- from pyspark.sql.functions import explode
4. It will create new column name keys and show all the keys in that perticular records.
      eg :- from pyspark.sql.functions import map_keys
            data = [('shabdali',{'hair':'black','eye':'brown'}),('sanjay',{'hair':'brown','eye':'blue'})]
            schema = StructType([\ StructField('name',StringType()),
                                 \ StructField('properties',MapType(StringType(),StringType()))])
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('keys', map_keys(df.properties))
            display(df1)
            df1.printSchema()
            df1.show(truncate=False)
5. It will create new column name values and show all the values in that perticular records.
      eg :- from pyspark.sql.functions import map_values
            data = [('shabdali',{'hair':'black','eye':'brown'}),('sanjay',{'hair':'brown','eye':'blue'})]
            schema = StructType([\ StructField('name',StringType()),
                                 \ StructField('properties',MapType(StringType(),StringType()))])   
            df = spark.createDataFrame(data,schema)
            df1 = df.withColumn('values', map_values(df.properties))
            display(df1)
            df1.printSchema()
            df1.show(truncate=False)


Video 17. Row() class in PySpark
                -> pyspark.sql.Row which is represented as a record/row in DataFrame, one can create a Row object by using named arguments or create a custom Row like class.

1. If you want to import Row from the library.
      eg :- from pyspark.sql import Row
2. help() Function :- This function help us how we can write a any syntax.
      eg :- help(Row)
3. We access/add 2 items together and will display together but both items should have same datatype that only it is possible.
   Here we got an error because one item is in string and one is in integer form.
      eg :- from pyspark.sql import Row
            row = Row('shabdali',2000)
            print(row[0] + ' ' + row[1])
4. We access/add 2 items together and will display together but both items should have same datatype that only it is possible.
   We convert integer datatype into string to access that items. We access the items with the help of indexs.
      eg :- from pyspark.sql import Row
            row = Row('shabdali',2000)
            print(row[0] + ' ' + str(row[1]))
5. We give names to each items. So we used names rather than index to acces the items. 
   Got error different datatype. 
      eg :- from pyspark.sql import Row
            row = Row(name = 'shabdali',salary = 2000)
            print(row.name + ' ' + row.salary)
6. We give names to each items. So we used names rather than index to acces the items. 
   Convert datatype and access items.
      eg :- from pyspark.sql import Row
            row = Row(name = 'shabdali',salary = 2000)
            print(row.name + ' ' + str(row.salary)) 
7. Access multiple rows dataframe.
      eg :- from pyspark.sql import Row
            row1 = Row(name = 'shabdali',salary = 20000000)
            row2 = Row(name = 'sanjay',salary = 10000000)
            data = [row1,row2]
            df = spark.createDataFrame(data)
            display(df)
            df.printSchema()
            df.show()
8. This is happen because Person1 is a object of Person. Person has a property called name using this row class.
      eg :- Person = Row('name','age')
            Person1 = Person('Shandali','25')
            Person2 = Person('Sanjay','100')
            print(Person1.name)
            print(Person1.age)
            print(Person2.name)
            print(Person1.age)
9. We had create a Dataframe.
      eg :- Person = Row('name','age')
            Person1 = Person('Shandali','25')
            Person2 = Person('Sanjay','100')
            Persondf = spark.createDataFrame([Person1,Person2])
            display(Persondf)
            Persondf.printSchema()
            Persondf.show()
10. We want to see datatype of the prop column that is struct type.
    We get 2 column that is name and prop.
      eg :- data = [Row(name = 'Shabdali', prop = Row(age = 25, gender = 'female')),
                  \ Row(name = 'Sarvesh', prop = Row(age = 24, gender = 'male'))]
            df = spark.createDataFrame(data)
            display(df)
            df.printSchema()
            df.show()

