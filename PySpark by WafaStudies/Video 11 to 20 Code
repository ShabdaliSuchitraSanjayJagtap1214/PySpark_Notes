Video 11. withColumnRenamed() usage in PySpark
                -> PySpark withColumnRenamed() is a Transformation Function of Dataframe which is used to change existing column name in Dataframe.
â€‹
1. If you want to import everything from this library and start the spark session
      eg :- from pyspark.sql import *
            spark=SparkSession.builder.appName('Dataframe11').getOrCreate()
2. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            display(df)
            df.show()
            df.printSchema()
3. help() Function :- This function help us how we can write a any syntax.
      eg :- help(df.withColumnRenamed)
4. Dataframe are immutable it means we can not change existing dataframe df and always whatever we function applied it will actually gunerate another Dataframe.
   Checking that it is realy immutable or not and the name of salary column has not change
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df.withColumnRenamed('salary','salary_amount')
            display(df)
            df.show()
            df.printSchema()
5. We rename the salary column name to salary_amount and updated data will store in new dataframe df1.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data=data,schema=columns)
            df1 = df.withColumnRenamed('salary','salary_amount')
            display(df1)
            df1.show()
            df1.printSchema()



Videp 12. StructType() and StructField() in PySpark
                -> PySpark StructField classes are used to programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns.
                -> PySpark StructType is a collection of StructField's.

1. Creating Dataframe with the help of List and but we do not have Column Name because we do not define schema and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            df = spark.createDataFrame(data)
            display(df)
            df.printSchema()
            df.show()
2. Creating Dataframe with the help of List and Column Name and we get Datatype of id is Long.
      eg :- data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            columns = ['id','name','salary']
            df = spark.createDataFrame(data,columns)
            display(df)
            df.printSchema()
            df.show()
3. If you want to import StructType, StructField, StringType, IntegerType from the library
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
4. Creating Dataframe with the help of List and Column Name by using StructType and StructField Function and define Datatype as well.
   Another way to create Dataframe.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
            data = [(1,'Shabdali','30000000'),(2,'Sanjay','10000000')]
            schema = StructType([\ StructField(name='id',dataType=IntegerType()),
                                 \ StructField(name='name',dataType=StringType()),
                                 \ StructField(name='salary',dataType=IntegerType())])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()
5. Creating Dataframe with the help of List and Column Name by using StructType and StructField Function and define Datatype as well.
   Another way to create Dataframe.
      eg :- from pyspark.sql.types import StructType, StructField, StringType, IntegerType
            data = [(1,('Shabdali','Jagtap'),'30000000'),(2,('Sanjay','Patil'),'10000000')]
            structName = StructType([\ StructField('firstName',StringType()),
                                     \ StructField('lastName',StringType())])
            schema = StructType([\ StructField(name='id',dataType=IntegerType()),
                                 \ StructField(name='name',dataType=structName),
                                 \ StructField(name='salary',dataType=IntegerType())])
            df = spark.createDataFrame(data,schema)
            display(df)
            df.printSchema()
            df.show()


Video 13. ArrayType Columns in PySpark










